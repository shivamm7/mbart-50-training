{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of mbart-50-en-mr.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3yAulnkiDK4"
      },
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git@master\n",
        "!pip install git+https://github.com/huggingface/datasets.git@master\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIfvtsXYjASk"
      },
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    MBartForConditionalGeneration, MBart50TokenizerFast, \n",
        "    Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "  )\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import random_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJcnel5IkXY3"
      },
      "source": [
        "# **Creating Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9XxSh2PJAdz"
      },
      "source": [
        "#path to train files\n",
        "with open(\"train.en\") as f1, open(\"train.mr\") as f2:\n",
        "    for src, tgt in zip(f1, f2):\n",
        "      train_data.append(\n",
        "          {\n",
        "              \"translation\": {\n",
        "                  \"en_XX\": src.strip(),\n",
        "                  \"mr_IN\": tgt.strip()\n",
        "              }\n",
        "          }\n",
        "      )\n",
        "print(f'total size of train data is {len(train_data)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlgUISazjyoS"
      },
      "source": [
        "#path to valid files\n",
        "valid_data = []\n",
        "with open(\"tun.en\") as f1, open(\"tun.mr\") as f2:\n",
        "    for src, tgt in zip(f1, f2):\n",
        "      valid_data.append(\n",
        "          {\n",
        "              \"translation\": {\n",
        "                  \"en_XX\": src.strip(),\n",
        "                  \"mr_IN\": tgt.strip()\n",
        "              }\n",
        "          }\n",
        "      )\n",
        "print(f'total size of valid data is {len(valid_data)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UupRpOagkEtj"
      },
      "source": [
        "def data_collator(features:list):\n",
        "\n",
        "  inputs = [f[\"translation\"][\"en_XX\"] for f in features]\n",
        "  labels = [f[\"translation\"][\"mr_IN\"] for f in features]\n",
        "  \n",
        "\n",
        "  #batch = tokenizer.prepare_seq2seq_batch(src_texts=inputs, src_lang=\"en_XX\", tgt_lang=\"mr_IN\", tgt_texts=labels, max_length=32, max_target_length=32)\n",
        "\n",
        "  #for k in batch:\n",
        "  #  batch[k] = torch.tensor(batch[k])\n",
        "  \n",
        "  input = tokenizer(inputs, return_tensors=\"pt\", max_length=32, truncation=True, padding=True)\n",
        "  with tokenizer.as_target_tokenizer():\n",
        "    label = tokenizer(labels, return_tensors=\"pt\", max_length=32, truncation=True, padding=True).input_ids\n",
        "  \n",
        "  batch = input\n",
        "  batch['labels'] = label\n",
        "\n",
        "  return batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o92M6U2dkcq0"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvdcOOZjkR5I"
      },
      "source": [
        "# initiating model, tokenizer\n",
        "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50\")\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"en_XX\", tgt_lang=\"mr_IN\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPsiTGM7OTIx",
        "outputId": "6e92da5b-ae1a-447e-a830-398b2a3c2a78"
      },
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total number of parameters: {total_params}\")\n",
        "print(f\"Total number of trainable parameters: {total_trainable_params}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of parameters: 610879488\n",
            "Total number of trainable parameters: 610879488\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA8CGDzBtqJC"
      },
      "source": [
        "# defining training related arguments\n",
        "args = Seq2SeqTrainingArguments(output_dir=\"en-mr\",\n",
        "                        do_train=True,\n",
        "                        do_eval=True,\n",
        "                        evaluation_strategy=\"epoch\",\n",
        "                        per_device_train_batch_size=16,\n",
        "                        per_device_eval_batch_size=16,\n",
        "                        learning_rate=5e-4,\n",
        "                        num_train_epochs=10,\n",
        "                        save_strategy=\"epoch\",\n",
        "                        gradient_accumulation_steps=4,\n",
        "                        eval_accumulation_steps=4,\n",
        "                        logging_dir=\"/logs\",\n",
        "                        save_total_limit=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6Vy9Qs3kURw"
      },
      "source": [
        "trainer = Seq2SeqTrainer(model=model, \n",
        "                args=args, \n",
        "                data_collator=data_collator, \n",
        "                train_dataset=train_data, \n",
        "                eval_dataset=valid_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8D70uc_kU0M"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}